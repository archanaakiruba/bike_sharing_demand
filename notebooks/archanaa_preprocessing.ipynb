{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import requests\n",
    "import holidays\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from bikesharing.params import *\n",
    "from bikesharing.ml_logic.data import get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing functions ##\n",
    "\n",
    "def pre_process_rental_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the rental DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Select relevant columns only\n",
    "    df = df[['STARTTIME', 'STARTLAT', 'STARTLON']].copy()\n",
    "\n",
    "    # Strip column names\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "    # Remove column 'Row'\n",
    "    df.drop(columns='Row', inplace=True, errors='ignore')\n",
    "\n",
    "    # Make string replacements\n",
    "    df_obj = df.select_dtypes(include='object')\n",
    "    df[df_obj.columns] = df_obj.applymap(lambda x: x.strip().replace(',', '.') if isinstance(x, str) else x)\n",
    "\n",
    "    # Handle datetime and numerical datatypes\n",
    "    df.STARTTIME = pd.to_datetime(df.STARTTIME)\n",
    "    df[['STARTLAT', 'STARTLON']] = df[['STARTLAT', 'STARTLON']].astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_polygons(polygons_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads polygons from a file and returns them as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        polygons_file (str): The path to the polygons file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The dictionary of polygons.\n",
    "    \"\"\"\n",
    "    polygons = {}\n",
    "    with open(polygons_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            polygons[row['district']] = Polygon(json.loads(row['coordinates']))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def get_district(rental_df: pd.DataFrame, polygons: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a spatial join between the rental DataFrame and polygons.\n",
    "\n",
    "    Args:\n",
    "        rental_df (pd.DataFrame): The rental DataFrame.\n",
    "        polygons (dict): The dictionary of polygons.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the spatial join result.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the polygons dictionary\n",
    "    polygons_df = pd.DataFrame.from_dict(polygons, orient='index', columns=['geometry'])\n",
    "    # Reset the index to make the 'district' column a regular column\n",
    "    polygons_df = polygons_df.reset_index().rename(columns={'index': 'district'})\n",
    "\n",
    "    # Create a GeoDataFrame from the polygons DataFrame\n",
    "    polygons_gdf = gpd.GeoDataFrame(polygons_df)\n",
    "    # Set the geometry column in the polygons_gdf GeoDataFrame\n",
    "    polygons_gdf.set_geometry('geometry', inplace=True)\n",
    "\n",
    "    # Create a GeoDataFrame from the point data\n",
    "    geometry = [Point(row['STARTLON'], row['STARTLAT']) for _, row in rental_df.iterrows()]\n",
    "    rental_gdf = gpd.GeoDataFrame(rental_df, geometry=geometry)\n",
    "    # Set the geometry column in the rental_gdf GeoDataFrame\n",
    "    rental_gdf.set_geometry('geometry', inplace=True)\n",
    "\n",
    "    # Perform the spatial join\n",
    "    rental_geo_df = gpd.sjoin(rental_gdf, polygons_gdf, predicate='within')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    rental_geo_df = rental_geo_df.drop(columns=['geometry', 'index_right'])\n",
    "\n",
    "    return rental_geo_df\n",
    "\n",
    "\n",
    "def encode_district_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes the district labels in the DataFrame using one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with encoded district labels.\n",
    "    \"\"\"\n",
    "    # Instantiate the OneHotEncoder\n",
    "    district_ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # Fit encoder\n",
    "    district_ohe.fit(df[['district']])\n",
    "\n",
    "    # Apply one-hot encoding and add the encoded columns to the DataFrame\n",
    "    encoded_columns = district_ohe.get_feature_names_out()\n",
    "    encoded_values = district_ohe.transform(df[['district']])\n",
    "    df_encoded = pd.DataFrame(encoded_values, columns=encoded_columns)\n",
    "\n",
    "    # Update the column names in df without the prefix 'district_'\n",
    "    column_names = [column.split('district_', 1)[-1] for column in df_encoded.columns]\n",
    "    df.columns = list(df.columns[:-len(encoded_columns)]) + column_names\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def group_rental_data_by_hour(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups the rental data by hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with rental data grouped by hour.\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    df['date'] = df['STARTTIME'].dt.date\n",
    "    df['year'] = df['STARTTIME'].dt.year\n",
    "    df['month'] = df['STARTTIME'].dt.month\n",
    "    df['hour'] = df['STARTTIME'].dt.hour\n",
    "    df['date_hour'] = df['STARTTIME'].dt.floor('H')\n",
    "\n",
    "    # Grouping by Hour\n",
    "    df_by_hour = df.groupby('date_hour').agg({\n",
    "        'RENTAL_IS_STATION': np.mean,\n",
    "        'year': np.mean,\n",
    "        'month': np.mean,\n",
    "        'hour': np.mean,\n",
    "        **{district: np.sum for district in df['district']}\n",
    "    }).reset_index()\n",
    "\n",
    "    return df_by_hour\n",
    "\n",
    "\n",
    "def get_weather_info(rental_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves weather information and merges it with the rental data.\n",
    "\n",
    "    Args:\n",
    "        rental_df (pd.DataFrame): The rental DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with merged rental and weather data.\n",
    "    \"\"\"\n",
    "    def fetch_weather_data(latitude, longitude, start_date, end_date):\n",
    "        # TODO: should be moved to env\n",
    "        url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "        params = {\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'hourly': ['temperature_2m', 'relativehumidity_2m', 'apparent_temperature', 'windspeed_10m', 'precipitation']\n",
    "        }\n",
    "        weather_data = requests.get(url, params=params).json()\n",
    "        df_weather = pd.DataFrame(weather_data['hourly'])\n",
    "        df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
    "\n",
    "        return df_weather\n",
    "\n",
    "    # Merge rental data with weather data\n",
    "    df_weather = fetch_weather_data()\n",
    "    merged_data = rental_df.merge(df_weather, left_on='date_hour', right_on='time', how='left').drop(columns='time')\n",
    "    merged_data['date_hour'] = pd.to_datetime(merged_data['date_hour'])\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def feature_extraction(data: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the input data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the features DataFrame (X) and the target DataFrame (y).\n",
    "    \"\"\"\n",
    "    data['date_hour'] = pd.to_datetime(data['date_hour'])\n",
    "\n",
    "    # Extract date from date_hour\n",
    "    data['date'] = data['date_hour'].dt.date\n",
    "\n",
    "    # Select features for X and y\n",
    "    X = data[['date', 'date_hour', 'year', 'month', 'hour', 'temperature_2m', 'relativehumidity_2m',\n",
    "              'apparent_temperature', 'windspeed_10m', 'precipitation']]\n",
    "    y = data[['RENTAL_IS_STATION']]\n",
    "    \n",
    "    # Add sine and cosine transformations of time-related features\n",
    "    time_features = ['hour', 'month', 'date_hour.dt.weekday']\n",
    "    for feature in time_features:\n",
    "        X[f'{feature}_sin'] = np.sin(2 * np.pi * X[feature] / X[feature].max())\n",
    "        X[f'{feature}_cos'] = np.cos(2 * np.pi * X[feature] / X[feature].max())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def scale_numeric_features(data: pd.DataFrame, numeric_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scales the numeric features in the given DataFrame using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "        numeric_features (list): List of column names representing the numeric features to scale.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with scaled numeric features.\n",
    "    \"\"\"\n",
    "    scaled_data = data.copy()\n",
    "\n",
    "    # Scale the numeric features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data[numeric_features] = scaler.fit_transform(scaled_data[numeric_features])\n",
    "\n",
    "    return scaled_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to fetch rental data from BigQuery\n",
    "query = f'''SELECT STARTTIME, STARTLAT, STARTLON\n",
    "         FROM `{GCP_PROJECT}.{BQ_DATASET}.raw_data_mvg`\n",
    "         WHERE STARTTIME >= '2022-01-01' AND STARTTIME <= '2022-12-31' \n",
    "         '''\n",
    "\n",
    "# Fetch the rental data from BigQuery\n",
    "rentals_df = get_raw_data(GCP_PROJECT, query, LOCAL_DATA_PATH)\n",
    "\n",
    "# Preprocess the rental data\n",
    "rental_df_processed = pre_process_rental_df(rentals_df)\n",
    "\n",
    "# TODO: Fetch polygons data from BigQuery\n",
    "polygons_file = pd.read_csv('.../raw_data/polygons.csv')\n",
    "\n",
    "# Load polygons from the file\n",
    "polygons = load_polygons(polygons_file)\n",
    "\n",
    "# Perform spatial join between rental data and polygons\n",
    "rental_geo_df = get_district(rental_df_processed, polygons)\n",
    "\n",
    "# Encode district labels using one-hot encoding\n",
    "encoded_rental_df = encode_district_label(rental_geo_df)\n",
    "\n",
    "# Group rental data by hour\n",
    "rental_df_by_hour = group_rental_data_by_hour(encoded_rental_df)\n",
    "\n",
    "# Retrieve weather information and merge with rental data\n",
    "rental_df_all = get_weather_info(rental_df_by_hour)\n",
    "\n",
    "# Perform feature engineering on the merged data\n",
    "X, y = feature_extraction(rental_df_all)\n",
    "\n",
    "# Scale the numeric features in X using StandardScaler\n",
    "X_scaled = scale_numeric_features(X)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
