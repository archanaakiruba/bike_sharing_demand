{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# colab\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# os.chdir allows you to change directories, like cd in the Terminal\n",
    "os.chdir('./drive/MyDrive/Colab Notebooks/project')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Sequence\n",
    "\n",
    "def get_folds(\n",
    "    df: pd.DataFrame,\n",
    "    fold_length: int,\n",
    "    fold_stride: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Overall dataframe\n",
    "        fold_length (int): How long each fold should be in rows\n",
    "        fold_stride (int): How many timesteps to move forward between taking each fold\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list where each fold is a dataframe within\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "\n",
    "def train_test_split(fold:pd.DataFrame,\n",
    "                     train_test_ratio: float,\n",
    "                     input_length: int) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"From a fold dataframe, take a train dataframe and test dataframe based on\n",
    "    the split ratio.\n",
    "    - df_train should contain all the timesteps until round(train_test_ratio * len(fold))\n",
    "    - df_test should contain all the timesteps needed to create all (X_test, y_test) tuples\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): A fold of timesteps\n",
    "        train_test_ratio (float): The ratio between train and test 0-1\n",
    "        input_length (int): How long each X_i will be\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: A tuple of two dataframes (fold_train, fold_test)\n",
    "    \"\"\"\n",
    "\n",
    "    # TRAIN SET\n",
    "    last_train_idx = round(train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_idx, :]\n",
    "\n",
    "    # TEST SET\n",
    "    first_test_idx = last_train_idx - input_length\n",
    "    fold_test = fold.iloc[first_test_idx:, :]\n",
    "\n",
    "    return (fold_train, fold_test)\n",
    "\n",
    "def get_Xi_yi(\n",
    "    fold:pd.DataFrame,\n",
    "    input_length:int,\n",
    "    output_length:int) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"given a fold, it returns one sequence (X_i, y_i) as based on the desired\n",
    "    input_length and output_length with the starting point of the sequence being chosen at random based\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): A single fold\n",
    "        input_length (int): How long each X_i should be\n",
    "        output_length (int): How long each y_i should be\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: A tuple of two dataframes (X_i, y_i)\n",
    "    \"\"\"\n",
    "\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(fold) - (input_length + output_length) + 1\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "    X_i = fold.iloc[random_start:random_start+input_length]\n",
    "    y_i = fold.iloc[random_start+input_length:\n",
    "                  random_start+input_length+output_length][TARGET]\n",
    "\n",
    "    return (X_i, y_i)\n",
    "\n",
    "def get_X_y(\n",
    "    fold:pd.DataFrame,\n",
    "    number_of_sequences:int,\n",
    "    input_length:int,\n",
    "    output_length:int) -> Tuple[np.array]:\n",
    "    \"\"\"Given a fold generate X and y based on the number of desired sequences\n",
    "    of the given input_length and output_length\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): Fold dataframe\n",
    "        number_of_sequences (int): The number of X_i and y_i pairs to include\n",
    "        input_length (int): Length of each X_i\n",
    "        output_length (int): Length of each y_i\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.array]: A tuple of numpy arrays (X, y)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(number_of_sequences):\n",
    "        (Xi, yi) = get_Xi_yi(fold, input_length, output_length)\n",
    "        X.append(Xi)\n",
    "        y.append(yi)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_X_y_strides(fold: pd.DataFrame, input_length: int, output_length: int, \n",
    "    sequence_stride: int) -> Tuple[np.array]:\n",
    "    \"\"\"slides through a `fold` Time Series (2D array) to create sequences of equal\n",
    "        * `input_length` for X,\n",
    "        * `output_length` for y,\n",
    "    using a temporal gap `sequence_stride` between each sequence\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): One single fold dataframe\n",
    "        input_length (int): Length of each X_i\n",
    "        output_length (int): Length of each y_i\n",
    "        sequence_stride (int): How many timesteps to take before taking the next X_i\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.array]: A tuple of numpy arrays (X, y)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(0, len(fold), sequence_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (i + input_length + output_length) >= len(fold):\n",
    "            break\n",
    "        X_i = fold.iloc[i:i + input_length, :]\n",
    "        y_i = fold.iloc[i + input_length:i + input_length + output_length, :][TARGET]\n",
    "        X.append(X_i)\n",
    "        y.append(y_i)\n",
    "\n",
    "    return (np.array(X), np.array(y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bikesharing.params import *\n",
    "# local\n",
    "data = pd.read_csv(f'{LOCAL_DATA_PATH}/processed/data_for_rnn_subset_of_districts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab\n",
    "data = pd.read_csv('./data_for_rnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = ['Altstadt-Lehel', 'Au - Haidhausen', \n",
    "       'Berg am Laim', 'Bogenhausen', 'Hadern', 'Laim', \n",
    "       'Ludwigsvorstadt-Isarvorstadt', 'Maxvorstadt', 'Milbertshofen-Am Hart',\n",
    "       'Moosach', 'Neuhausen-Nymphenburg', 'Obergiesing', \n",
    "       'Obersendling', 'Pasing', 'Pasing-Obermenzing', 'Ramersdorf-Perlach',\n",
    "       'Schwabing-Freimann', 'Schwabing-West', 'Schwanthalerhöhe', 'Sendling',\n",
    "       'Sendling-Westpark', 'Thalkirchen', 'Trudering',\n",
    "       'Trudering-Riem', 'Untergiesing', 'Untergiesing-Harlaching',\n",
    "       'Aubing-Lochhausen-Langwied', 'Harlaching', 'Südgiesing', 'Hasenbergl-Lerchenau Ost', \n",
    "       'Obermenzing', 'Feldmoching', 'Untermenzing-Allach', 'Lochhausen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_LENGTH = 17520\n",
    "FOLD_STRIDE = 2184\n",
    "TRAIN_TEST_RATIO = 0.8\n",
    "INPUT_LENGTH = 672 # 24 h * 28 d\n",
    "OUTPUT_LENGTH = 24\n",
    "SEQUENCE_STRIDE = 1\n",
    "TARGET = districts\n",
    "N_TARGETS = len(districts)\n",
    "N_FEATURES = 13\n",
    "N_TRAIN = 8000 # number_of_sequences_train\n",
    "N_TEST =  2000 # number_of_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of folds: 5\n"
     ]
    }
   ],
   "source": [
    "folds = get_folds(data, FOLD_LENGTH, FOLD_STRIDE)\n",
    "print(f'number of folds: {len(folds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fold_train, fold_test) = train_test_split(folds[0], TRAIN_TEST_RATIO, INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fold_train, fold_test) = train_test_split(data, TRAIN_TEST_RATIO, INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_i, y_train_i = get_Xi_yi(fold_train, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_test_i, y_test_i = get_Xi_yi(fold_test, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_train_i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(fold_train, N_TRAIN, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_test, y_test = get_X_y(fold_test, N_TEST, INPUT_LENGTH, OUTPUT_LENGTH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronological Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y_strides(fold_train, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)\n",
    "X_test, y_test = get_X_y_strides(fold_test, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0,0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 18:19:39.615893: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-11 18:19:39.675053: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-11 18:19:40.021834: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-11 18:19:40.024925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 18:19:41.983365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(X_train, y_train):\n",
    "    # E1D1\n",
    "    # n_features ==> no of features at each timestep in the data.\n",
    "    #\n",
    "    encoder_inputs = layers.Input(shape=X_train[0].shape)\n",
    "\n",
    "    norm_layer = layers.Normalization()\n",
    "    norm_layer.adapt(X_train)\n",
    "    norm = norm_layer(encoder_inputs)\n",
    "\n",
    "    encoder_l1 = layers.LSTM(100, return_state=True)\n",
    "    encoder_outputs1 = encoder_l1(norm)\n",
    "\n",
    "    encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "    #\n",
    "    decoder_inputs = layers.RepeatVector(OUTPUT_LENGTH)(encoder_outputs1[0])\n",
    "\n",
    "    #\n",
    "    decoder_l1 = layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "    decoder_outputs1 = layers.TimeDistributed(layers.Dense(y_train[0].shape[1]))(decoder_l1)\n",
    "\n",
    "    #\n",
    "    model_e1d1 = models.Model(encoder_inputs,decoder_outputs1)\n",
    "    \n",
    "    # 2 - Compiler\n",
    "    # ======================    \n",
    "    adam = optimizers.Adam(learning_rate=0.02)    \n",
    "    model_e1d1.compile(loss='mse', optimizer=adam, metrics=[\"mae\"])\n",
    "    \n",
    "    return model_e1d1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    # --- LOSS: MSE --- \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- METRICS:MAE ---\n",
    "    \n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "                        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def fit_model(model: keras.Model, verbose=1) -> Tuple[keras.Model, dict]:\n",
    "\n",
    "    es = EarlyStopping(monitor = \"val_loss\",\n",
    "                      patience = 5,\n",
    "                      mode = \"min\",\n",
    "                      restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split = 0.3,\n",
    "                        shuffle = False,\n",
    "                        batch_size = 32,\n",
    "                        epochs = 50,\n",
    "                        callbacks = [es],\n",
    "                        verbose = False)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Initialising the RNN model\n",
    "# ====================================\n",
    "\n",
    "model = init_model(X_train, y_train)\n",
    "model.summary()\n",
    "\n",
    "# 2 - Training\n",
    "# ====================================\n",
    "model, history = fit_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/2023-06-11_14:15_rnn_subset_of_districts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline_and_lstm(data: pd.DataFrame):\n",
    "    '''\n",
    "    This function cross-validates \n",
    "    - the \"last seen value\" baseline model\n",
    "    - the RNN model\n",
    "    '''\n",
    "    \n",
    "    #list_of_mae_baseline_model = []\n",
    "    list_of_mse_recurrent_model = []\n",
    "    hist = []\n",
    "    \n",
    "    # 0 - Creating folds\n",
    "    # =========================================    \n",
    "    folds = get_folds(data, FOLD_LENGTH, FOLD_STRIDE)\n",
    "    \n",
    "    for fold_id, fold in enumerate(folds):\n",
    "        \n",
    "        # 1 - Train/Test split the current fold\n",
    "        # =========================================\n",
    "        (fold_train, fold_test) = train_test_split(fold, TRAIN_TEST_RATIO, INPUT_LENGTH)                   \n",
    "\n",
    "        X_train, y_train = get_X_y_strides(fold_train, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)\n",
    "        X_test, y_test = get_X_y_strides(fold_test, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)\n",
    "        #X_train, y_train = get_X_y(fold_train, N_TRAIN, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        #X_test, y_test = get_X_y(fold_test, N_TEST, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        \n",
    "        # 2 - Modelling\n",
    "        # =========================================\n",
    "        \n",
    "        ##### Baseline Model\n",
    "        #baseline_model = init_baseline()\n",
    "        #mae_baseline = baseline_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "        #list_of_mae_baseline_model.append(mae_baseline)\n",
    "        #print(\"-\"*50)\n",
    "        #print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")\n",
    "\n",
    "        ##### LSTM Model\n",
    "        model = init_model(X_train, y_train)\n",
    "        es = EarlyStopping(monitor = \"loss\",\n",
    "                           mode = \"min\",\n",
    "                           patience = 5, \n",
    "                           restore_best_weights = True)\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            validation_split = 0.3,\n",
    "                            shuffle = False,\n",
    "                            batch_size = 32,\n",
    "                            epochs = 50,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "        hist.append(history)\n",
    "        res = model.evaluate(X_test, y_test, verbose=0)\n",
    "        mse_lstm = res[1]\n",
    "        list_of_mse_recurrent_model.append(mse_lstm)\n",
    "        print(f\"MSE LSTM fold n°{fold_id} = {round(mse_lstm, 2)}\")\n",
    "        \n",
    "        ##### Comparison LSTM vs Baseline for the current fold\n",
    "        #print(f\"🏋🏽‍♂️ improvement over baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")\n",
    "\n",
    "    #return list_of_mae_baseline_model, list_of_mae_recurrent_model\n",
    "    return list_of_mse_recurrent_model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses, histories = cross_validate_baseline_and_lstm(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = pd.read_csv('./eval_data_for_rnn.csv')\n",
    "data_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Aubing-Lochhausen-Langwied', 'Harlaching', 'Südgiesing', 'Hasenbergl-Lerchenau Ost', 'Obermenzing', 'Feldmoching', \n",
    "             'Untermenzing-Allach', 'Lochhausen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval.drop(columns=drop_cols, inplace=True)\n",
    "data_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data[districts])\n",
    "data_eval_scaled = data_eval.copy()\n",
    "data_eval_scaled[districts] = scaler.transform(data_eval[districts])\n",
    "data_eval_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of X and y_true for evaluation \n",
    "X_eval, y_true = [], []\n",
    "\n",
    "for i in range(0,len(data_eval)-INPUT_LENGTH-OUTPUT_LENGTH,OUTPUT_LENGTH):\n",
    "    Xi = data_eval_scaled.loc[i:i+INPUT_LENGTH-1]\n",
    "    yi = data_eval[i+INPUT_LENGTH:i+INPUT_LENGTH+OUTPUT_LENGTH][districts]\n",
    "    X_eval.append(Xi.to_numpy())\n",
    "    y_true.append(yi.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = np.array(X_eval)\n",
    "y_true = np.array(y_true)\n",
    "print(f'X_eval: {X_eval.shape}, y_true: {y_true.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval[0,0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_eval, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(17.5079)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_eval)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = []\n",
    "for i in range(y_pred.shape[0]):\n",
    "  y_df.append(pd.DataFrame(y_pred[i], columns=districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_total = pd.concat(y_df, axis=0).reset_index().drop(columns='index')\n",
    "y_pred_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_total = data_eval[INPUT_LENGTH:-5][districts].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(4,1, figsize=(20,15))\n",
    "districts=['Altstadt-Lehel', 'Maxvorstadt', 'Lochhausen', 'Bogenhausen']\n",
    "for i in range(4):\n",
    "  sns.lineplot(x=y_true_total.index, y=districts[i], data=y_true_total, ax=ax[i])\n",
    "  sns.lineplot(x=y_pred_total.index, y=districts[i], data=y_pred_total, ax=ax[i], color='red')\n",
    "  plt.title(districts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,1, figsize=(20,15))\n",
    "for i in range(4):\n",
    "  residuals = y_true_total[[districts[i]]] - y_pred_total[[districts[i]]]\n",
    "  sns.lineplot(x=residuals.index, y=districts[i], data=residuals, ax=ax[i])\n",
    "  sns.lineplot(x=residuals.index, y=0, data=residuals, ax=ax[i], color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,1, figsize=(20,15))\n",
    "for i in range(4):\n",
    "  residuals = y_true_total[[districts[i]]] - y_pred_total[[districts[i]]]\n",
    "  sns.histplot(x=districts[i], data=residuals, ax=ax[i])\n",
    "  #sns.lineplot(x=residuals.index, y=0, data=residuals, ax=ax[i], color='red');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike_sharing_demand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
